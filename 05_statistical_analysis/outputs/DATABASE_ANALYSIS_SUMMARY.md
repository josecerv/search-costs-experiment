# Database Impact Analysis - Final Summary

## What We Measured

For each seminar, we calculated what percentage of their speakers came from the URM database that would have been shown to that department. The database was tailored - each department only saw URM faculty from their peer universities (Â±20 ranks, minimum 40 universities).

## Key Results

### Overall Impact
- **Treatment**: 0.131% of speakers came from the database (19 out of ~11,295)
- **Control**: 0.165% of speakers came from the database (15 out of ~11,873)
- **Not statistically different** (p=0.659)

### Binary Outcome: Did ANY speaker come from the database?
- **Treatment**: 19 out of 811 seminars (2.3%)
- **Control**: 15 out of 845 seminars (1.8%)
- **Not statistically different** (p=0.489)

### Black Speakers Specifically
- **Treatment**: 10 seminars had a Black speaker from database (1.2%)
- **Control**: 4 seminars had a Black speaker from database (0.5%)
- **2.6x higher in treatment** but not quite significant (p=0.111)

## Why So Low?

1. **Only 75 out of 550 database faculty (13.6%) ever appeared as speakers anywhere**
2. **Seminars have ~14 speakers on average**, so even one database speaker is only ~7% of the seminar
3. **The specific faculty in the databases rarely give seminars**

## Interpretation

The curated URM databases had **minimal direct impact** on speaker selection. Only 2.3% of treatment seminars selected any speaker from the database. While there's a directional effect for Black speakers (2.6x more likely in treatment), the absolute numbers are very small.

This suggests that any diversity improvements from the intervention likely came through **indirect mechanisms** (awareness, accountability, signaling) rather than direct selection from the provided lists.